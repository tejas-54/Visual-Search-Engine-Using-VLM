# Visual-Search-Engine-Using-VLM
This project implements a visual search engine leveraging state-of-the-art vision-language models (VLMs) to retrieve relevant images based on textual queries or sample images. The system embeds both text and images into a shared representation space, allowing for semantic search across modalities.


Features
Multi-Modal Search: Query using either text descriptions or example images
Semantic Understanding: Find images based on conceptual meaning, not just keywords
Efficient Retrieval: Fast similarity search using approximate nearest neighbors
Intuitive Web Interface: Simple UI for uploading images or entering text queries

Installation:

Prerequisites
Python 3.8+
PyTorch 1.9+
FAISS
Flask
