# Visual-Search-Engine-Using-VLM
This project implements a visual search engine leveraging state-of-the-art vision-language models (VLMs) to retrieve relevant images based on textual queries or sample images. The system embeds both text and images into a shared representation space, allowing for semantic search across modalities.

# Uses in the Apparel Domain  
This visual search engine enables apparel shoppers to find clothing items by submitting either text descriptions ("red floral maxi dress") or uploading reference images of desired styles. The system analyzes the query using vision-language models to understand visual and textual features, then retrieves the most visually similar items from the product catalog. This approach eliminates the frustration of text-only searches that miss visual nuances, reduces search time by showing highly relevant results, and helps customers discover products that precisely match their style preferences even when they struggle to describe fashion details in words.  

## Features:  
__Multi-Modal Search:__ Query using either text descriptions or example images  
__Semantic Understanding:__ Find images based on conceptual meaning, not just keywords  
__Efficient Retrieval:__ Fast similarity search using approximate nearest neighbors  
__Intuitive Web Interface:__ Simple UI for uploading images or entering text queries    


## Setup:  
Clone this repository:  
```
git clone <repository-url>
cd visual-search-vlm
```
